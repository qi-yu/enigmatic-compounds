{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the annotated data set will be processed for the Exploratory Data Analysis and the implementation of the autmatic detection task using a Keras language model (https://keras.io/guides/sequential_model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:25:25.744612Z",
     "iopub.status.busy": "2023-10-18T07:25:25.744168Z",
     "iopub.status.idle": "2023-10-18T07:25:25.750323Z",
     "shell.execute_reply": "2023-10-18T07:25:25.749355Z",
     "shell.execute_reply.started": "2023-10-18T07:25:25.744572Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:25:29.857183Z",
     "iopub.status.busy": "2023-10-18T07:25:29.856865Z",
     "iopub.status.idle": "2023-10-18T07:25:57.308911Z",
     "shell.execute_reply": "2023-10-18T07:25:57.307915Z",
     "shell.execute_reply.started": "2023-10-18T07:25:29.857159Z"
    }
   },
   "outputs": [],
   "source": [
    "#downloading the large German model\n",
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:26:20.898512Z",
     "iopub.status.busy": "2023-10-18T07:26:20.898147Z",
     "iopub.status.idle": "2023-10-18T07:26:22.867082Z",
     "shell.execute_reply": "2023-10-18T07:26:22.866276Z",
     "shell.execute_reply.started": "2023-10-18T07:26:20.898483Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading the model\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating different DataFrames for different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:39:49.090076Z",
     "iopub.status.busy": "2023-10-18T07:39:49.089686Z",
     "iopub.status.idle": "2023-10-18T07:39:49.197183Z",
     "shell.execute_reply": "2023-10-18T07:39:49.196187Z",
     "shell.execute_reply.started": "2023-10-18T07:39:49.090046Z"
    }
   },
   "outputs": [],
   "source": [
    "#reading in the data set\n",
    "df_og = pd.read_csv('/kaggle/input/completed-annotation/all_annotated.csv')\n",
    "\n",
    "df_og.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:39:51.708923Z",
     "iopub.status.busy": "2023-10-18T07:39:51.708612Z",
     "iopub.status.idle": "2023-10-18T07:39:51.728362Z",
     "shell.execute_reply": "2023-10-18T07:39:51.726923Z",
     "shell.execute_reply.started": "2023-10-18T07:39:51.708900Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating sub-dataframe for either enigmatic or descriptive compounds\n",
    "#for easier data manipulation\n",
    "\n",
    "#splitting the DataFrame based on 'Annotation' value\n",
    "df_en = df_og[df_og['Annotation'] == 1]\n",
    "df_de = df_og[df_og['Annotation'] == 0]\n",
    "\n",
    "print(\"DataFrame with Annotation == 1:\")\n",
    "print(df_en.head())\n",
    "\n",
    "print(\"\\nDataFrame with Annotation == 0:\")\n",
    "print(df_de.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the compounds (removing hyphens, lowercasing and lemmatizing them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:39:55.282253Z",
     "iopub.status.busy": "2023-10-18T07:39:55.281886Z",
     "iopub.status.idle": "2023-10-18T07:39:55.391164Z",
     "shell.execute_reply": "2023-10-18T07:39:55.389850Z",
     "shell.execute_reply.started": "2023-10-18T07:39:55.282224Z"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the text, i.e., making it lowercase, removing hyphens from the compounds, etc.\n",
    "import re\n",
    "\n",
    "#making a copy of the original DataFrame\n",
    "df_mod = df_og.copy()\n",
    "\n",
    "#removing hyphens from hyphenated compounds\n",
    "def modify_compound(row):\n",
    "    word = row['Compound']\n",
    "    if '-' in word:\n",
    "        #remove hyphen and merge the two constituents into one\n",
    "        modified_word = re.sub(r'(\\w+)-(\\w+)', lambda match: match.group(1).capitalize() + match.group(2).lower(), word)\n",
    "    else:\n",
    "        #handle all uppercase words\n",
    "        modified_word = word.lower().capitalize()\n",
    "    return modified_word\n",
    "\n",
    "#apply the function to the 'Compound' column\n",
    "df_mod['Compound'] = df_mod.apply(modify_compound, axis=1)\n",
    "\n",
    "print(df_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T07:39:58.764636Z",
     "iopub.status.busy": "2023-10-18T07:39:58.764051Z",
     "iopub.status.idle": "2023-10-18T07:41:30.411062Z",
     "shell.execute_reply": "2023-10-18T07:41:30.409503Z",
     "shell.execute_reply.started": "2023-10-18T07:39:58.764608Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemmatizing each word to obtain the underlying form (i.e., remove plural ending, etc.)\n",
    "df_mod['Compound'] = df_mod['Compound'].apply(lambda x: [token.lemma_ for token in nlp(x)][0])\n",
    "print(df_mod.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modifying the Enigmatic DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:12:31.978506Z",
     "iopub.status.busy": "2023-10-18T08:12:31.977982Z",
     "iopub.status.idle": "2023-10-18T08:12:31.999574Z",
     "shell.execute_reply": "2023-10-18T08:12:31.997563Z",
     "shell.execute_reply.started": "2023-10-18T08:12:31.978473Z"
    }
   },
   "outputs": [],
   "source": [
    "#copying the original DataFrame\n",
    "df_mod_en = df_en.copy()\n",
    "\n",
    "#removing hyphens from hyphenated compounds\n",
    "def modify_compound(row):\n",
    "    word = row['Compound']\n",
    "    if '-' in word:\n",
    "        #remove hyphen and merge the two constituents into one\n",
    "        modified_word = re.sub(r'(\\w+)-(\\w+)', lambda match: match.group(1).capitalize() + match.group(2).lower(), word)\n",
    "    else:\n",
    "        #handle all uppercase words\n",
    "        modified_word = word.lower().capitalize()\n",
    "    return modified_word\n",
    "\n",
    "#apply the function to the 'Compound' column\n",
    "df_mod_en['Compound'] = df_mod_en.apply(modify_compound, axis=1)\n",
    "\n",
    "print(df_mod_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:12:34.453093Z",
     "iopub.status.busy": "2023-10-18T08:12:34.452770Z",
     "iopub.status.idle": "2023-10-18T08:12:37.844616Z",
     "shell.execute_reply": "2023-10-18T08:12:37.843407Z",
     "shell.execute_reply.started": "2023-10-18T08:12:34.453071Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemmatizing each word to obtain the underlying form (i.e., remove plural ending, etc.)\n",
    "df_mod_en['Compound'] = df_mod_en['Compound'].apply(lambda x: [token.lemma_ for token in nlp(x)][0])\n",
    "print(df_mod_en)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the descriptive DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:15:11.430040Z",
     "iopub.status.busy": "2023-10-18T08:15:11.429736Z",
     "iopub.status.idle": "2023-10-18T08:15:11.534507Z",
     "shell.execute_reply": "2023-10-18T08:15:11.533100Z",
     "shell.execute_reply.started": "2023-10-18T08:15:11.430009Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mod_de = df_de.copy()\n",
    "\n",
    "def modify_compound(row):\n",
    "    word = row['Compound']\n",
    "    if '-' in word:\n",
    "        modified_word = re.sub(r'(\\w+)-(\\w+)', lambda match: match.group(1).capitalize() + match.group(2).lower(), word)\n",
    "    else:\n",
    "        modified_word = word.lower().capitalize()\n",
    "    return modified_word\n",
    "\n",
    "df_mod_de['Compound'] = df_mod_de.apply(modify_compound, axis=1)\n",
    "\n",
    "print(df_mod_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:15:14.303903Z",
     "iopub.status.busy": "2023-10-18T08:15:14.303589Z",
     "iopub.status.idle": "2023-10-18T08:16:40.818705Z",
     "shell.execute_reply": "2023-10-18T08:16:40.817338Z",
     "shell.execute_reply.started": "2023-10-18T08:15:14.303874Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemmatizing each word to obtain the underlying form (i.e., remove plural ending, etc.)\n",
    "df_mod_de['Compound'] = df_mod_de['Compound'].apply(lambda x: [token.lemma_ for token in nlp(x)][0])\n",
    "print(df_mod_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following part of the notebook, I am performing an Exploratory Data Analysis (EDA) to get a better insight into the data set and the statistical distribution of the compounds. Specifically, the following distributions will be calculated:\n",
    "\n",
    "* the total number of compounds (enigmatic vs. descriptive) in each newspaper\n",
    "* the most frequent compounds (enigmatic vs. descriptive) overall and for each newspaper\n",
    "* the distribution of compounds (enigmatic vs. descriptive) across the years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the compounds were normalized above, different forms of the same compound are counted as 1 instance of the base form when calculating the most frequent compounds. This is necessary for an accurate calculation, as for example, Flüchtlingskrise, Flüchtlings-Krise, and FLÜCHTLINGSKRISE should all be counted as an instance of the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of enigmatic vs. descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Number of enigmatic vs. descriptive Compounds by Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:40.277618Z",
     "iopub.status.busy": "2023-10-18T08:18:40.277220Z",
     "iopub.status.idle": "2023-10-18T08:18:40.289503Z",
     "shell.execute_reply": "2023-10-18T08:18:40.288590Z",
     "shell.execute_reply.started": "2023-10-18T08:18:40.277590Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculating the total distribution of enigmatic compounds from each newspaper\n",
    "#grouping by newspaper source and counting the occurrences with size()\n",
    "comp_en_news = df_en.groupby('Source')['Compound'].size()\n",
    "\n",
    "print(\"Total number of enigmatic Compounds from each newspaper:\", \"\\n\", comp_en_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:42.804552Z",
     "iopub.status.busy": "2023-10-18T08:18:42.804147Z",
     "iopub.status.idle": "2023-10-18T08:18:42.813585Z",
     "shell.execute_reply": "2023-10-18T08:18:42.812316Z",
     "shell.execute_reply.started": "2023-10-18T08:18:42.804526Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculating the total distribution of descriptive compounds from each newspaper\n",
    "#grouping by newspaper source and counting the occurrences with size()\n",
    "comp_de_news = df_de.groupby('Source')['Compound'].size()\n",
    "\n",
    "print(\"Total number of descriptive Compounds from each newspaper:\", \"\\n\", comp_de_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Number of enigmatic vs. descriptive Compounds by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:45.271136Z",
     "iopub.status.busy": "2023-10-18T08:18:45.270829Z",
     "iopub.status.idle": "2023-10-18T08:18:45.280101Z",
     "shell.execute_reply": "2023-10-18T08:18:45.279190Z",
     "shell.execute_reply.started": "2023-10-18T08:18:45.271114Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculating the distribution of compounds from each year\n",
    "comp_year_en = df_en.groupby('Year')['Compound'].size()\n",
    "\n",
    "print(\"Total number of enigmatic Compounds from each year:\", \"\\n\", comp_year_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:48.147582Z",
     "iopub.status.busy": "2023-10-18T08:18:48.147259Z",
     "iopub.status.idle": "2023-10-18T08:18:48.155643Z",
     "shell.execute_reply": "2023-10-18T08:18:48.154301Z",
     "shell.execute_reply.started": "2023-10-18T08:18:48.147555Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculating the distribution of compounds from each year\n",
    "comp_year_de = df_de.groupby('Year')['Compound'].size()\n",
    "\n",
    "print(\"Total number of descriptive Compounds from each year:\", \"\\n\", comp_year_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent enigmatic compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enigmatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:50.391992Z",
     "iopub.status.busy": "2023-10-18T08:18:50.391484Z",
     "iopub.status.idle": "2023-10-18T08:18:50.406583Z",
     "shell.execute_reply": "2023-10-18T08:18:50.405432Z",
     "shell.execute_reply.started": "2023-10-18T08:18:50.391967Z"
    }
   },
   "outputs": [],
   "source": [
    "#determining the most frequent enigmatic compound\n",
    "#calculating the frequency of each compound\n",
    "en_counts = df_mod_en['Compound'].value_counts()\n",
    "\n",
    "#getting the 10 most frequent compounds\n",
    "en_all = en_counts.nlargest(10)\n",
    "\n",
    "print(\"The 10 most frequent enigmatic compounds are:\", \"\\n\", en_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:52.946128Z",
     "iopub.status.busy": "2023-10-18T08:18:52.945721Z",
     "iopub.status.idle": "2023-10-18T08:18:53.182295Z",
     "shell.execute_reply": "2023-10-18T08:18:53.181262Z",
     "shell.execute_reply.started": "2023-10-18T08:18:52.946098Z"
    }
   },
   "outputs": [],
   "source": [
    "#visualizing the results using a horizontal bar plot\n",
    "en_all_sorted = en_all.sort_values(ascending=True)\n",
    "\n",
    "plt.barh(en_all_sorted.index, en_all_sorted.values, color='pink')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Compound')\n",
    "plt.title('Top 10 Most Frequent Compounds Overall')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:56.512893Z",
     "iopub.status.busy": "2023-10-18T08:18:56.511714Z",
     "iopub.status.idle": "2023-10-18T08:18:56.529535Z",
     "shell.execute_reply": "2023-10-18T08:18:56.528109Z",
     "shell.execute_reply.started": "2023-10-18T08:18:56.512863Z"
    }
   },
   "outputs": [],
   "source": [
    "#determining the most frequent descriptive compound\n",
    "#calculating the frequency of each compound\n",
    "de_counts = df_mod_de['Compound'].value_counts()\n",
    "\n",
    "#getting the 10 most frequent compounds\n",
    "de_all = de_counts.nlargest(10)\n",
    "\n",
    "print(\"The 10 most frequent descriptive compounds are:\", \"\\n\", de_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:18:59.051492Z",
     "iopub.status.busy": "2023-10-18T08:18:59.051160Z",
     "iopub.status.idle": "2023-10-18T08:18:59.253399Z",
     "shell.execute_reply": "2023-10-18T08:18:59.251864Z",
     "shell.execute_reply.started": "2023-10-18T08:18:59.051461Z"
    }
   },
   "outputs": [],
   "source": [
    "de_all_sorted = de_all.sort_values(ascending=True)\n",
    "\n",
    "plt.barh(de_all_sorted.index, de_all_sorted.values, color='pink')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Compound')\n",
    "plt.title('Top 10 Most Frequent descriptive Compounds')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common compounds by newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enigmatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:02.092339Z",
     "iopub.status.busy": "2023-10-18T08:19:02.092023Z",
     "iopub.status.idle": "2023-10-18T08:19:02.117650Z",
     "shell.execute_reply": "2023-10-18T08:19:02.116727Z",
     "shell.execute_reply.started": "2023-10-18T08:19:02.092319Z"
    }
   },
   "outputs": [],
   "source": [
    "#determining the most frequent Compounds for each newspaper\n",
    "#sorting the compounds by newspaper and counting them\n",
    "comp_en_news = df_mod_en.groupby('Source')['Compound'].value_counts()\n",
    "\n",
    "#reset_index(drop=True) removes the outer index level, i.e., the newspaper names\n",
    "year_en = comp_en_news.groupby(level=0).nlargest(5).reset_index(level=0, drop=True)\n",
    "\n",
    "print(\"The most frequent enigmatic compounds for each newspaper:\", \"\\n\", year_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:04.576008Z",
     "iopub.status.busy": "2023-10-18T08:19:04.575641Z",
     "iopub.status.idle": "2023-10-18T08:19:04.687070Z",
     "shell.execute_reply": "2023-10-18T08:19:04.686077Z",
     "shell.execute_reply.started": "2023-10-18T08:19:04.575969Z"
    }
   },
   "outputs": [],
   "source": [
    "#determining the most frequent Compounds for each newspaper\n",
    "comp_de_news = df_mod_de.groupby('Source')['Compound'].value_counts()\n",
    "\n",
    "#getting the most frequent compounds for each newspaper\n",
    "#reset_index(drop=True) removes the outer index level, i.e., the newspaper names\n",
    "year_de = comp_de_news.groupby(level=0).nlargest(5).reset_index(level=0, drop=True)\n",
    "\n",
    "print(\"The most frequent compounds for each newspaper:\", \"\\n\", year_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common compounds by newspaper and by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enigmatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:07.758864Z",
     "iopub.status.busy": "2023-10-18T08:19:07.758561Z",
     "iopub.status.idle": "2023-10-18T08:19:07.805985Z",
     "shell.execute_reply": "2023-10-18T08:19:07.804571Z",
     "shell.execute_reply.started": "2023-10-18T08:19:07.758842Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_en_years = df_mod_en.groupby(['Year', 'Source'])['Compound'].value_counts()\n",
    "\n",
    "years_en = comp_en_years.groupby(level=[0, 1]).nlargest(10).reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "print(\"The 5 most frequent enigmatic compounds for each newspaper and each year:\", years_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:11.667838Z",
     "iopub.status.busy": "2023-10-18T08:19:11.667282Z",
     "iopub.status.idle": "2023-10-18T08:19:11.838312Z",
     "shell.execute_reply": "2023-10-18T08:19:11.837287Z",
     "shell.execute_reply.started": "2023-10-18T08:19:11.667811Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_de_years = df_mod_de.groupby(['Year', 'Source'])['Compound'].value_counts()\n",
    "\n",
    "years_de = comp_de_years.groupby(level=[0, 1]).nlargest(5).reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "print(\"The 5 most frequent descriptive compounds for each newspaper and each year:\", years_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat Analysis\n",
    "\n",
    "In the next step, a chi-square test is performed to determine if there are significant differences in the distribution of enigmatic and descriptive compounds between newspapers. For this, I am using the library SciPy (https://scipy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:14.305876Z",
     "iopub.status.busy": "2023-10-18T08:19:14.305371Z",
     "iopub.status.idle": "2023-10-18T08:19:14.328005Z",
     "shell.execute_reply": "2023-10-18T08:19:14.326990Z",
     "shell.execute_reply.started": "2023-10-18T08:19:14.305853Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "#creating a contingency table\n",
    "contingency_table = pd.crosstab(df_og['Source'], df_og['Annotation'])\n",
    "\n",
    "#performing the Chi-squared test\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "print(\"Chi-squared value:\", chi2)\n",
    "print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an alpha level of 0.05 and a very low p-value of 5.06e-72, there is a significant association between the class of compounds and the newspapers. Thus the Null Hypothesis can be rejected with very high certainty due to an extremely low probability of the Null Hypothesis being true. This means, it can be predicted when either a descriptive or an enigmatic compound is used in a newspaper. This is in line with previous observations of the reporting style of tabloid media, such as BILD, in contrast to the more serious and less sensational reporting style of quality newspapers, as FAZ and SZ (see Chapter 4). To sum up, this adds a nuance to the differences in the use of enigmatic compounds between the three newspapers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for the Automated Detection Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though common practice in the preprocessing step in NLP tasks, the data was manually annotated, thus there will be no missing values (NAs) that need to be removed in either DataFrame.\n",
    "\n",
    "Due to the nature of the ECs, this data does not yield a huge number of them. This became evident already during the manual annotation, but to visualize this, the difference in class balance is illustrated in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:17.848038Z",
     "iopub.status.busy": "2023-10-18T08:19:17.847575Z",
     "iopub.status.idle": "2023-10-18T08:19:18.041750Z",
     "shell.execute_reply": "2023-10-18T08:19:18.040412Z",
     "shell.execute_reply.started": "2023-10-18T08:19:17.848004Z"
    }
   },
   "outputs": [],
   "source": [
    "#plotting the data distribution according to the classes 0 = 'descriptive' and 1 = 'enigmatic'\n",
    "counts = df_mod['Annotation'].value_counts()\n",
    "counts.plot(kind='bar', color=['orange', 'blue'])\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Classes')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with imbalanced data is generally not recommended, as it can lead to a preference for the majority class (in this case the descriptive compounds) as well as overfitting. \n",
    "There are two alternatives to deal with imbalanced data: over- and undersampling. In cases of extreme class imbalances, oversampling would create an impractical number of fabricated examples, thus undersampling poses as a practical alternative. This means, the data set of the descriptive compounds was drastically reduced to match with the minority class (see also Section 5.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:21.062949Z",
     "iopub.status.busy": "2023-10-18T08:19:21.062601Z",
     "iopub.status.idle": "2023-10-18T08:19:21.068708Z",
     "shell.execute_reply": "2023-10-18T08:19:21.067879Z",
     "shell.execute_reply.started": "2023-10-18T08:19:21.062920Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking the length of the descriptive DataFrame\n",
    "len(df_mod_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:23.203251Z",
     "iopub.status.busy": "2023-10-18T08:19:23.202615Z",
     "iopub.status.idle": "2023-10-18T08:19:23.209841Z",
     "shell.execute_reply": "2023-10-18T08:19:23.208803Z",
     "shell.execute_reply.started": "2023-10-18T08:19:23.203225Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking the length of the enigmatic DataFrame\n",
    "len(df_mod_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:25.358701Z",
     "iopub.status.busy": "2023-10-18T08:19:25.358378Z",
     "iopub.status.idle": "2023-10-18T08:19:25.367937Z",
     "shell.execute_reply": "2023-10-18T08:19:25.366732Z",
     "shell.execute_reply.started": "2023-10-18T08:19:25.358681Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing random samples from the descriptive DataFrame to match with the size\n",
    "#of the enigmatic DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "len_en = len(df_mod_en)\n",
    "\n",
    "# Randomly sample from df_mod_de to match the length of df_mod_en\n",
    "if len(df_mod_de) > len_en:\n",
    "    # Set a seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    # Randomly shuffle the larger DataFrame\n",
    "    df_mod_de = df_mod_de.sample(n=len_en, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:31.333520Z",
     "iopub.status.busy": "2023-10-18T08:19:31.333138Z",
     "iopub.status.idle": "2023-10-18T08:19:31.340004Z",
     "shell.execute_reply": "2023-10-18T08:19:31.338588Z",
     "shell.execute_reply.started": "2023-10-18T08:19:31.333485Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking whether the descriptive DataFrame has the correct length\n",
    "len(df_mod_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the DataFrames have already been cleaned and lemmatized, they will now be tokenized and further processed so that they can be used as input for the language model. Generally, this includes converting the tokenized text to sequences and then padding those, and splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:33.607032Z",
     "iopub.status.busy": "2023-10-18T08:19:33.606724Z",
     "iopub.status.idle": "2023-10-18T08:19:33.615546Z",
     "shell.execute_reply": "2023-10-18T08:19:33.614666Z",
     "shell.execute_reply.started": "2023-10-18T08:19:33.607009Z"
    }
   },
   "outputs": [],
   "source": [
    "#combining the information from both the descriptive as well as enigmatic \n",
    "#DataFrames into the text column of the new DataFrame df_LM\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "df_LM = pd.concat([df_mod_de[['Compound', 'Title', 'Annotation']], df_mod_en[['Compound', 'Title', 'Annotation']]])\n",
    "\n",
    "# Reset the index of the new dataframe\n",
    "df_LM.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tokenization, a general process for NLP tasks is implemented, including defining the input dimensions and using the built-in keras Tokenizer (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) used for language models.\n",
    "As the labels are already binary, there is no need to one-hot encode the labels for this model, which is one of the requirements to be able to implement them. Thus, the labels are merely transformed into a Numpy array, so ensure that all of the tensors have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:36.358872Z",
     "iopub.status.busy": "2023-10-18T08:19:36.358515Z",
     "iopub.status.idle": "2023-10-18T08:19:36.419989Z",
     "shell.execute_reply": "2023-10-18T08:19:36.419125Z",
     "shell.execute_reply.started": "2023-10-18T08:19:36.358845Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#defining the lengths of the input\n",
    "#using the first 100 words of each headline (the headlines are not longer than 100 words)\n",
    "max_len = 100\n",
    "#using the first 10k words from the data set\n",
    "vocab_size = 10000\n",
    "\n",
    "#providing both the titles as well as the compounds as input data\n",
    "text = df_LM['Title'] + df_LM['Compound']\n",
    "#transforming the ratings from the column 'fraudulent' into a numpy array\n",
    "labels = df_LM['Annotation'].values\n",
    "\n",
    "#tokenizing\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "#creating the mapping between words and integers\n",
    "#to convert new text into a sequence of integers -> model input\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)  \n",
    "\n",
    "#creating a dictionary where each word is a key \n",
    "#and the index is the corresponding value\n",
    "word_index = tokenizer.word_index  \n",
    "\n",
    "#padding the sequences, so that they are the same length\n",
    "data = pad_sequences(sequences, maxlen=max_len) \n",
    "\n",
    "#shuffling the data so that it is random\n",
    "indices = np.arange(data.shape[0]) \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data split, the entire data set is first split into training and test sets. Then, the training set is split into a smaller training and validation set. This process is most commonly used for this type of classification task, as it retains comparability. The function train_test_split from the sklearn library (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is used for this. It has predefined parameters according to which it splits the data.\n",
    "\n",
    "The random_state parameter was set to 1, so for every iteration of the model, the data split stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:42.121838Z",
     "iopub.status.busy": "2023-10-18T08:19:42.121545Z",
     "iopub.status.idle": "2023-10-18T08:19:42.129290Z",
     "shell.execute_reply": "2023-10-18T08:19:42.128696Z",
     "shell.execute_reply.started": "2023-10-18T08:19:42.121813Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = labels\n",
    "X = data\n",
    "\n",
    "#splitting the data into test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 1)\n",
    "\n",
    "#creating a validation set from the training data set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Compiling\n",
    "As mentioned before, the data set consists of two classes: enigmatic (1) and descriptive (0). Thus, I am using Logistic Regression, as it is specifically designed for binary classification. LR calculates a probability estimate between 0 and 1 for each item according to which class it belongs. The closer the probability score is to 1, the higher the confidence of the model that a specific instance belongs to class 1, i.e., the enigmatic compounds, in this case.\n",
    "Logistic Regression is easy to interpret due to this simple classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:44.615209Z",
     "iopub.status.busy": "2023-10-18T08:19:44.614911Z",
     "iopub.status.idle": "2023-10-18T08:19:45.049890Z",
     "shell.execute_reply": "2023-10-18T08:19:45.048809Z",
     "shell.execute_reply.started": "2023-10-18T08:19:44.615188Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout \n",
    "\n",
    "#setting the random seed so that the weights stay the same\n",
    "np.random.seed(115)\n",
    "\n",
    "#defining the model\n",
    "#setting the output dimensions to 20\n",
    "out_dim=20\n",
    "\n",
    "#using a sequential model\n",
    "model = Sequential(name = \"model\")\n",
    "#using an embedding, LSTM, and dense layer\n",
    "model.add(Embedding(vocab_size,out_dim,input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(10 )))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#compiling the model, i.e., setting the parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:19:47.825251Z",
     "iopub.status.busy": "2023-10-18T08:19:47.824940Z",
     "iopub.status.idle": "2023-10-18T08:20:00.516181Z",
     "shell.execute_reply": "2023-10-18T08:20:00.515153Z",
     "shell.execute_reply.started": "2023-10-18T08:19:47.825228Z"
    }
   },
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:20:09.010219Z",
     "iopub.status.busy": "2023-10-18T08:20:09.009877Z",
     "iopub.status.idle": "2023-10-18T08:20:09.347160Z",
     "shell.execute_reply": "2023-10-18T08:20:09.345925Z",
     "shell.execute_reply.started": "2023-10-18T08:20:09.010194Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'k', linestyle = 'dashdot', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'm', linewidth = '3', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'k', linestyle = 'dashdot', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', linewidth = '3', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:20:13.797636Z",
     "iopub.status.busy": "2023-10-18T08:20:13.797286Z",
     "iopub.status.idle": "2023-10-18T08:20:13.940769Z",
     "shell.execute_reply": "2023-10-18T08:20:13.939090Z",
     "shell.execute_reply.started": "2023-10-18T08:20:13.797607Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Evaluating the model on the test data:')\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T08:20:16.213886Z",
     "iopub.status.busy": "2023-10-18T08:20:16.213550Z",
     "iopub.status.idle": "2023-10-18T08:20:17.091784Z",
     "shell.execute_reply": "2023-10-18T08:20:17.090020Z",
     "shell.execute_reply.started": "2023-10-18T08:20:16.213861Z"
    }
   },
   "outputs": [],
   "source": [
    "#predicting on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.around(y_pred, decimals=0)\n",
    "\n",
    "print('confusion matrix:',confusion_matrix(y_test, y_pred))\n",
    "print('classification report:',classification_report(y_test,y_pred, target_names=['0', '1']))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3851452,
     "sourceId": 6718015,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
